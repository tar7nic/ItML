{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "AIML\n",
      "Group\n",
      "Project\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "text = \"This is AIML Group Project\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "for token in tokens:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1f5f4b7a130>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "i\n",
      "am\n",
      "tarun\n"
     ]
    }
   ],
   "source": [
    "text = \"hello i am tarun\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This project shows application of AIML.\n",
      "\n",
      "Our topic for Application of AIML is NLP.\n",
      "\n",
      "NLP is Natural Language Processing.\n",
      "\n",
      "NLP is a very useful and interesting topic.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text2 = \"This project shows application of AIML. Our topic for Application of AIML is NLP. NLP is Natural Language Processing. NLP is a very useful and interesting topic.\"\n",
    "doc = nlp(text2)\n",
    "for sentences in doc.sents:\n",
    "  print(sentences)\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today\n",
      "\n",
      "is\n",
      "\n",
      "a\n",
      "\n",
      "very\n",
      "\n",
      "humid\n",
      "\n",
      "day\n",
      "\n",
      ".\n",
      "\n",
      "We\n",
      "\n",
      "have\n",
      "\n",
      "to\n",
      "\n",
      "walk\n",
      "\n",
      "2.5\n",
      "\n",
      "kilometers\n",
      "\n",
      "in\n",
      "\n",
      "such\n",
      "\n",
      "a\n",
      "\n",
      "humid\n",
      "\n",
      "weather\n",
      "\n",
      ".\n",
      "\n",
      "Wifi\n",
      "\n",
      "is\n",
      "\n",
      "not\n",
      "\n",
      "working\n",
      "\n",
      "in\n",
      "\n",
      "the\n",
      "\n",
      "hostel\n",
      "\n",
      ".\n",
      "\n",
      "That\n",
      "\n",
      "'s\n",
      "\n",
      "why\n",
      "\n",
      "we\n",
      "\n",
      "are\n",
      "\n",
      "using\n",
      "\n",
      "Google\n",
      "\n",
      "Colab\n",
      "\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text3 = \"today is a very humid day. We have to walk 2.5 kilometers in such a humid weather. Wifi is not working in the hostel. That's why we are using Google Colab.\"\n",
    "tokens = nltk.word_tokenize(text3)\n",
    "for token in tokens:\n",
    "  print(token)\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"today humid day . We walk 2.5 kilometers humid weather . Wifi working hostel . That 's using Google Colab .\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "my_stopwords = set(stopwords.words('english'))\n",
    "new_tokens= []\n",
    "\n",
    "text = \"today is a very humid day. We have to walk 2.5 kilometers in such a humid weather. Wifi is not working in the hostel. That's why we are using Google Colab.\"\n",
    "all_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "for token in all_tokens:\n",
    "  if token not in my_stopwords:\n",
    "    new_tokens.append(token)\n",
    "\n",
    "\" \".join(new_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamood\n",
      "\n",
      "Hamood\n",
      "\n",
      "Hamood\n",
      "\n",
      "Habibi\n",
      "\n",
      "Hamood\n",
      "\n",
      "Habibi\n",
      "\n",
      "Hamood\n",
      "\n",
      "Hamood\n",
      "\n",
      "Habibi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text3 = \"Hamood Hamood Hamood Habibi Hamood Habibi Hamood Hamood Habibi\"\n",
    "tokens = nltk.word_tokenize(text3)\n",
    "for token in tokens:\n",
    "  print(token)\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Apple Inc., Label: ORG\n",
      "Entity: Steve Jobs, Label: PERSON\n",
      "Entity: Cupertino, Label: GPE\n",
      "Entity: California, Label: GPE\n",
      "Entity: India, Label: GPE\n",
      "Entity: John, Label: PERSON\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def perform_ner(text):\n",
    "  #Process the text with Spacy NLP pipeline\n",
    "  doc = nlp(text)\n",
    "\n",
    "  #Iterate through entities and print their text and label\n",
    "  for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n",
    "\n",
    "#Example text\n",
    "text5 = \"Apple Inc. was founded by Steve Jobs and has its headquarters in Cupertino, California. India is a big country. His name is John.\"\n",
    "\n",
    "#Perform NER on the example text\n",
    "perform_ner(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: 'I loved the movie. It was amazing.'\n",
      "Sentiment: Positive\n",
      "\n",
      "Text 2: 'The food was terrible and the service was bad.'\n",
      "Sentiment: Negative\n",
      "\n",
      "Text 3: 'The weather is beautiful today.'\n",
      "Sentiment: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_data = [\n",
    "    \"I loved the movie. It was amazing.\",\n",
    "    \"The food was terrible and the service was bad.\",\n",
    "    \"The weather is beautiful today.\"\n",
    "]\n",
    "\n",
    "# Predefined lists of positove and negative words\n",
    "positive_words = [\"loved\", \"amazing\", \"beautiful\"]\n",
    "negative_words = [\"terrible\", \"bad\"]\n",
    "\n",
    "# Perform sentiment analysis based on word lists\n",
    "def analyze_sentiment(text):\n",
    "  words = text.lower().split()\n",
    "  positive_count = sum(word in positive_words for word in words)\n",
    "  negative_count = sum(word in negative_words for word in words)\n",
    "\n",
    "  if positive_count > negative_count:\n",
    "    return \"Positive\"\n",
    "  elif negative_count > positive_count:\n",
    "    return \"Negative\"\n",
    "  else:\n",
    "    return \"Neutral\"\n",
    "\n",
    "# Analyze sentiment for each piece of text\n",
    "for i, text in enumerate(text_data):\n",
    "  sentiment = analyze_sentiment(text)\n",
    "  print(f\"Text {i + 1}: '{text}'\")\n",
    "  print(f\"Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Polarity: 0.5875\n",
      "Sentiment Subjectivity: 0.875\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Hypothetical text\n",
    "text = \"I woke up feeling incredibly happy today. The sun is shining, and I'm excited about the day ahead.\"\n",
    "\n",
    "#Perform sentiment analysis\n",
    "blob = TextBlob(text)\n",
    "sentiment = blob.sentiment\n",
    "\n",
    "#Print sentiment polarity and subjectivity\n",
    "print(\"Sentiment Polarity:\", sentiment.polarity)\n",
    "print(\"Sentiment Subjectivity:\", sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Verb\tPast Tense\tPresent Continuous\n",
      "=============================================\n",
      "walk\t\twalked\t\twalking\n",
      "jump\t\tjumped\t\tjumping\n",
      "play\t\tplayed\t\tplaying\n",
      "dance\t\tdanced\t\tdancing\n",
      "act\t\tacted\t\tacting\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def convert_to_past_tense(verb):\n",
    "    if verb.endswith('e'):\n",
    "        return verb + 'd'\n",
    "    elif verb.endswith('y'):\n",
    "        return verb + 'ed'\n",
    "    elif re.match(r'.*[aeiou][^aeiou]$', verb):\n",
    "        return verb + verb[-1] + 'ed'\n",
    "    else:\n",
    "        return verb + 'ed'\n",
    "\n",
    "def convert_to_present_continuous(verb):\n",
    "    if verb.endswith('e'):\n",
    "        return verb[:-1] + 'ing'\n",
    "    elif verb.endswith('y'):\n",
    "        return verb + 'ing'\n",
    "    elif re.match(r'.*[aeiou][^aeiou]$', verb):\n",
    "        return verb + verb[-1] + 'ing'\n",
    "    else:\n",
    "        return verb + 'ing'\n",
    "\n",
    "def main():\n",
    "    verbs = [\"walk\", \"jump\", \"play\", \"dance\", \"act\"]\n",
    "\n",
    "    print(\"Original Verb\\tPast Tense\\tPresent Continuous\")\n",
    "    print(\"=\"*45)\n",
    "\n",
    "    for verb in verbs:\n",
    "        doc = nlp(verb)\n",
    "        if doc[0].pos_ == \"VERB\":\n",
    "            past_tense = convert_to_past_tense(verb)\n",
    "            present_continuous = convert_to_present_continuous(verb)\n",
    "            print(f\"{verb}\\t\\t{past_tense}\\t\\t{present_continuous}\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text_7 = \"\"\n",
    "\n",
    "blob = TextBlob(text_7)\n",
    "sentiment = blob.sentiment\n",
    "\n",
    "sentiment.polarity\n",
    "# sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
